{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1vgKRvVQe_21"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh3D-q5ghJck",
        "outputId": "0626a580-21f0-4542-fbb0-8b23e9f46740"
      },
      "outputs": [],
      "source": [
        "# !pip install line_profiler==3.3.1\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jYyHrV2fqCf",
        "outputId": "0db7eca8-90bd-4806-b22d-92f2160d67de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmfN8_vafyQH",
        "outputId": "673fabf5-11a3-49bd-8596-579b60f1d8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /root/nltk_data/corpora/wordnet.zip\n",
            "   creating: /root/nltk_data/corpora/wordnet/\n",
            "  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/README  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"
          ]
        }
      ],
      "source": [
        "! unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XKWaIOWWf8ij"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSxgCNpbgVTg",
        "outputId": "2210d2cb-b5f7-4359-b55d-fbb040d86804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmBkzRn_gYJp",
        "outputId": "174f4e8d-f333-4633-dd79-61bb3a131d16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAX_LENGTH = 30\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tH5KjZsOimdA"
      },
      "outputs": [],
      "source": [
        "train= pd.read_csv('./train.csv')\n",
        "test= pd.read_csv(\"./test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhKRC69kk8KX",
        "outputId": "c473c22a-8045-4b4c-ebfc-30f590f9ac50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.target.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OP8nzsmNsM"
      },
      "source": [
        "# Preprocessing without optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oAsCo9JAlAtz"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r' ', string)\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    # remove URLS \n",
        "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
        "    # remove emoji's\n",
        "    sentence = remove_emoji(sentence)\n",
        "    # remove punctuation\n",
        "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
        "    # remove double spaces\n",
        "    sentence = sentence.replace('  ',\"\")\n",
        "    \n",
        "    return sentence.strip()\n",
        "\n",
        "def remove_stopwords(tokens, stopwords):\n",
        "    clean_token = [word for word in tokens if word not in stopwords]\n",
        "    return clean_token\n",
        "\n",
        "# lemmalization \n",
        "def lemmatize(tokens, lemma):\n",
        "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SBZmRoIelHRi"
      },
      "outputs": [],
      "source": [
        "# fix sentence length\n",
        "def trunc_padding(sentence):\n",
        "    modify_sentence = sentence.copy()\n",
        "    if len(modify_sentence) >= MAX_LENGTH:\n",
        "        modify_sentence = modify_sentence[:MAX_LENGTH]\n",
        "    else:\n",
        "        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n",
        "    return modify_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oGaap-xtlI8b"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def processing(df, stopwords, lemma):\n",
        "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
        "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
        "    # tokenization\n",
        "    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
        "    # remove stopwords\n",
        "    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    # lemmalization \n",
        "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
        "    # sentence length before padding\n",
        "    df['length'] = df['text'].apply(lambda x: len(x))\n",
        "    # fix sentence length\n",
        "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
        "    # sentence length after padding\n",
        "    df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ehPU3YtglLQp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-07 s\n",
            "\n",
            "Total time: 3.35363 s\n",
            "\n",
            "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_17644\\1383951964.py\n",
            "Are you sure you are running this program from the same directory\n",
            "that you ran the profiler from?\n",
            "Continuing without the function's contents.\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "     4                                           \n",
            "     5         1      52942.0  52942.0      0.2  \n",
            "     6         1     902872.0 902872.0      2.7  \n",
            "     7                                           \n",
            "     8         1      80908.0  80908.0      0.2  \n",
            "     9                                           \n",
            "    10         1    1381567.0 1381567.0      4.1  \n",
            "    11                                           \n",
            "    12         1   30905428.0 30905428.0     92.2  \n",
            "    13                                           \n",
            "    14         1      32408.0  32408.0      0.1  \n",
            "    15                                           \n",
            "    16         1     148931.0 148931.0      0.4  \n",
            "    17                                           \n",
            "    18         1      31231.0  31231.0      0.1"
          ]
        }
      ],
      "source": [
        "%lprun -f processing processing(train, stopwords, lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bG6W7c3elN6l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-07 s\n",
            "\n",
            "Total time: 0.377591 s\n",
            "\n",
            "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_17644\\1383951964.py\n",
            "Are you sure you are running this program from the same directory\n",
            "that you ran the profiler from?\n",
            "Continuing without the function's contents.\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "     4                                           \n",
            "     5         1      21408.0  21408.0      0.6  \n",
            "     6         1     323353.0 323353.0      8.6  \n",
            "     7                                           \n",
            "     8         1      39792.0  39792.0      1.1  \n",
            "     9                                           \n",
            "    10         1     611524.0 611524.0     16.2  \n",
            "    11                                           \n",
            "    12         1    2684292.0 2684292.0     71.1  \n",
            "    13                                           \n",
            "    14         1      17390.0  17390.0      0.5  \n",
            "    15                                           \n",
            "    16         1      61179.0  61179.0      1.6  \n",
            "    17                                           \n",
            "    18         1      16977.0  16977.0      0.4"
          ]
        }
      ],
      "source": [
        "%lprun -f processing processing(test, stopwords, lemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiA9W9O0mjyB"
      },
      "source": [
        "# Preprocessing with Cython optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LNvP5cFlmo1H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%%cython \n",
        "# cython: linetrace=True\n",
        "# cython: binding=True\n",
        "# distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
        "# cython: profile=True\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import itertools\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "cdef seed_everything(int seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(42)\n",
        "cdef int MAX_LENGTH = 30\n",
        "cdef object device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cdef clean_sentence(object sentence):\n",
        "    # remove URLS \n",
        "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
        "    # remove emoji's\n",
        "    cdef object emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    sentence = emoji_pattern.sub(r' ', sentence)\n",
        "\n",
        "    # remove punctuation\n",
        "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
        "    # remove double spaces\n",
        "    sentence = sentence.replace('  ',\"\")\n",
        "    \n",
        "    return sentence.strip()\n",
        "\n",
        "cdef remove_stopwords(list tokens, object stopwords):\n",
        "    cdef list clean_token = [word for word in tokens if word not in stopwords]\n",
        "    return clean_token\n",
        "\n",
        "# lemmalization \n",
        "cdef lemmatize(list tokens, object lemma):\n",
        "    cdef list lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "cdef trunc_padding(list sentence):\n",
        "    sentence = list(itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH)) # fix sentence length\n",
        "    return sentence\n",
        "\n",
        "cdef object stopwords = nltk.corpus.stopwords.words('english')\n",
        "cdef object lemma = WordNetLemmatizer()\n",
        "\n",
        "def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
        "    df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
        "    df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
        "    # tokenization 123123\n",
        "    df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
        "    # remove stopwords\n",
        "    df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    # lemmalization \n",
        "    df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
        "    # sentence length before padding\n",
        "    df['length'] = df['text'].map(lambda x: len(x))\n",
        "    # fix sentence length\n",
        "    df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
        "    # sentence length after padding\n",
        "    df['length_padding'] = df['text'].map(lambda x: len(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nSbrGYwnIlD",
        "outputId": "8bc03fb8-7e22-4ba8-cfba-4119b72b7884"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-07 s\n",
            "\n",
            "Total time: 3.26317 s\n",
            "File: C:\\Users\\czy\\.ipython\\cython\\_cython_magic_9b41f3ce78be3cc6a616579f23709c5e.pyx\n",
            "Function: processing at line 76\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
            "    77         1      51025.0  51025.0      0.2      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
            "    78         1     866269.0 866269.0      2.7      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
            "    79                                               # tokenization 123123\n",
            "    80         1      80244.0  80244.0      0.2      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
            "    81                                               # remove stopwords\n",
            "    82         1      94349.0  94349.0      0.3      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
            "    83                                               # lemmalization \n",
            "    84         1   31393296.0 31393296.0     96.2      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
            "    85                                               # sentence length before padding\n",
            "    86         1      25869.0  25869.0      0.1      df['length'] = df['text'].map(lambda x: len(x))\n",
            "    87                                               # fix sentence length\n",
            "    88         1      97880.0  97880.0      0.3      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
            "    89                                               # sentence length after padding\n",
            "    90         1      22784.0  22784.0      0.1      df['length_padding'] = df['text'].map(lambda x: len(x))"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "train= pd.read_csv('./train.csv')\n",
        "test= pd.read_csv(\"./test.csv\")\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemma = WordNetLemmatizer()\n",
        "%reload_ext line_profiler\n",
        "%lprun -f processing processing(train, stopwords, lemma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iOnVIG-Kqywg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-07 s\n",
            "\n",
            "Total time: 0.323276 s\n",
            "File: C:\\Users\\czy\\.ipython\\cython\\_cython_magic_9b41f3ce78be3cc6a616579f23709c5e.pyx\n",
            "Function: processing at line 76\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
            "    77         1      24311.0  24311.0      0.8      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
            "    78         1     303374.0 303374.0      9.4      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
            "    79                                               # tokenization 123123\n",
            "    80         1      57234.0  57234.0      1.8      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
            "    81                                               # remove stopwords\n",
            "    82         1      51662.0  51662.0      1.6      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
            "    83                                               # lemmalization \n",
            "    84         1    2729184.0 2729184.0     84.4      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
            "    85                                               # sentence length before padding\n",
            "    86         1      13026.0  13026.0      0.4      df['length'] = df['text'].map(lambda x: len(x))\n",
            "    87                                               # fix sentence length\n",
            "    88         1      41765.0  41765.0      1.3      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
            "    89                                               # sentence length after padding\n",
            "    90         1      12205.0  12205.0      0.4      df['length_padding'] = df['text'].map(lambda x: len(x))"
          ]
        }
      ],
      "source": [
        "%lprun -f processing processing(test, stopwords, lemma)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlB2L-DLwVe0"
      },
      "source": [
        "# Comparison\n",
        "processing training data without optimization takes:  3.35363 s <br>\n",
        "processing training data with Cython optimization takes:  3.26317 s <br>\n",
        "The speed up is : 3.35363 / 3.26317 ≈ 1.03 <br>\n",
        "The relative speed up is 3 %\n",
        "<br><br>\n",
        "processing testing data without optimization takes:  0.377591 s<br>\n",
        "processing testing data with Cython optimization takes:  0.323276 s <br>\n",
        "The speed up is : 0.377591 / 0.323276 ≈ 1.17<br>\n",
        "The relative speed up is 17 %\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
