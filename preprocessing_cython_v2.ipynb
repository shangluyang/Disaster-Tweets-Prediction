{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1vgKRvVQe_21"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install line_profiler==3.3.1\n",
        "%load_ext line_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh3D-q5ghJck",
        "outputId": "2719b81b-d2d3-4fe7-a6a8-9252fcea9801"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: line_profiler==3.3.1 in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: IPython>=0.13 in /usr/local/lib/python3.10/dist-packages (from line_profiler==3.3.1) (7.34.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (0.18.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (3.0.38)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (67.7.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython>=0.13->line_profiler==3.3.1) (0.1.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython>=0.13->line_profiler==3.3.1) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython>=0.13->line_profiler==3.3.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython>=0.13->line_profiler==3.3.1) (0.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jYyHrV2fqCf",
        "outputId": "6dda77d0-042c-443b-8d2e-30e3734a61ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/"
      ],
      "metadata": {
        "id": "FmfN8_vafyQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8488fc8d-204b-4fca-95e4-528cdba84c50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /root/nltk_data/corpora/wordnet.zip\n",
            "   creating: /root/nltk_data/corpora/wordnet/\n",
            "  inflating: /root/nltk_data/corpora/wordnet/lexnames  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adv.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.verb  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/cntlist.rev  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.adj  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/LICENSE  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/citation.bib  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/noun.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/verb.exc  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/README  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.sense  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/data.adv  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/index.noun  \n",
            "  inflating: /root/nltk_data/corpora/wordnet/adj.exc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed"
      ],
      "metadata": {
        "id": "XKWaIOWWf8ij"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSxgCNpbgVTg",
        "outputId": "393b73cc-22db-4797-9433-19383622c003"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 30\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmBkzRn_gYJp",
        "outputId": "f6b3e8a2-9088-41f0-8038-b4ee763d38cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train= pd.read_csv('/content/drive/MyDrive/Disaster-Tweets-Prediction-main/train.csv')\n",
        "test= pd.read_csv(\"/content/drive/MyDrive/Disaster-Tweets-Prediction-main/test.csv\")"
      ],
      "metadata": {
        "id": "tH5KjZsOimdA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhKRC69kk8KX",
        "outputId": "c7d48216-f1cd-46c7-d5a9-281347074825"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing without optimization"
      ],
      "metadata": {
        "id": "C6OP8nzsmNsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r' ', string)\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    # remove URLS \n",
        "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
        "    # remove emoji's\n",
        "    sentence = remove_emoji(sentence)\n",
        "    # remove punctuation\n",
        "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
        "    # remove double spaces\n",
        "    sentence = sentence.replace('  ',\"\")\n",
        "    \n",
        "    return sentence.strip()\n",
        "\n",
        "def remove_stopwords(tokens, stopwords):\n",
        "    clean_token = [word for word in tokens if word not in stopwords]\n",
        "    return clean_token\n",
        "\n",
        "# lemmalization \n",
        "def lemmatize(tokens, lemma):\n",
        "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "oAsCo9JAlAtz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix sentence length\n",
        "def trunc_padding(sentence):\n",
        "    modify_sentence = sentence.copy()\n",
        "    if len(modify_sentence) >= MAX_LENGTH:\n",
        "        modify_sentence = modify_sentence[:MAX_LENGTH]\n",
        "    else:\n",
        "        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n",
        "    return modify_sentence"
      ],
      "metadata": {
        "id": "SBZmRoIelHRi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def processing(df, stopwords, lemma):\n",
        "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
        "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
        "    # tokenization\n",
        "    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
        "    # remove stopwords\n",
        "    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    # lemmalization \n",
        "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
        "    # sentence length before padding\n",
        "    df['length'] = df['text'].apply(lambda x: len(x))\n",
        "    # fix sentence length\n",
        "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
        "    # sentence length after padding\n",
        "    df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "oGaap-xtlI8b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%lprun -f processing processing(train, stopwords, lemma)"
      ],
      "metadata": {
        "id": "ehPU3YtglLQp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "Timer unit: 1e-06 s\n",
        "\n",
        "Total time: 7.58081 s\n",
        "File: <ipython-input-11-0655b95bc61f>\n",
        "Function: processing at line 4\n",
        "\n",
        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
        "==============================================================\n",
        "     4                                           def processing(df, stopwords, lemma):\n",
        "     5         1      10126.0  10126.0      0.1      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
        "     6         1     137293.0 137293.0      1.8      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
        "     7                                               # tokenization\n",
        "     8         1      13126.0  13126.0      0.2      df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
        "     9                                               # remove stopwords\n",
        "    10         1     141971.0 141971.0      1.9      df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    11                                               # lemmalization \n",
        "    12         1    7250607.0 7250607.0     95.6      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
        "    13                                               # sentence length before padding\n",
        "    14         1       5602.0   5602.0      0.1      df['length'] = df['text'].apply(lambda x: len(x))\n",
        "    15                                               # fix sentence length\n",
        "    16         1      17704.0  17704.0      0.2      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
        "    17                                               # sentence length after padding\n",
        "    18         1       4379.0   4379.0      0.1      df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "vLPIU-UQmU_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%lprun -f processing processing(test, stopwords, lemma)"
      ],
      "metadata": {
        "id": "bG6W7c3elN6l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "Timer unit: 1e-06 s\n",
        "\n",
        "Total time: 0.59442 s\n",
        "File: <ipython-input-11-0655b95bc61f>\n",
        "Function: processing at line 4\n",
        "\n",
        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
        "==============================================================\n",
        "     4                                           def processing(df, stopwords, lemma):\n",
        "     5         1       5410.0   5410.0      0.9      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
        "     6         1      52004.0  52004.0      8.7      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
        "     7                                               # tokenization\n",
        "     8         1       5951.0   5951.0      1.0      df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
        "     9                                               # remove stopwords\n",
        "    10         1      72079.0  72079.0     12.1      df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    11                                               # lemmalization \n",
        "    12         1     446276.0 446276.0     75.1      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
        "    13                                               # sentence length before padding\n",
        "    14         1       2922.0   2922.0      0.5      df['length'] = df['text'].apply(lambda x: len(x))\n",
        "    15                                               # fix sentence length\n",
        "    16         1       6969.0   6969.0      1.2      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
        "    17                                               # sentence length after padding\n",
        "    18         1       2809.0   2809.0      0.5      df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "OPb3pxBSmhx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with Cython optimization"
      ],
      "metadata": {
        "id": "WiA9W9O0mjyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train= pd.read_csv('/content/drive/MyDrive/Disaster-Tweets-Prediction-main/train.csv')\n",
        "test= pd.read_csv(\"/content/drive/MyDrive/Disaster-Tweets-Prediction-main/test.csv\")"
      ],
      "metadata": {
        "id": "o_t79y3Uqkv-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZYS-vFhgaSi",
        "outputId": "924307d8-b334-42da-93dc-e692aee8a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Our Deeds are the Reason of this #earthquake M...\n",
              "1                  Forest fire near La Ronge Sask. Canada\n",
              "2       All residents asked to 'shelter in place' are ...\n",
              "3       13,000 people receive #wildfires evacuation or...\n",
              "4       Just got sent this photo from Ruby #Alaska as ...\n",
              "                              ...                        \n",
              "7608    Two giant cranes holding a bridge collapse int...\n",
              "7609    @aria_ahrary @TheTawniest The out of control w...\n",
              "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
              "7611    Police investigating after an e-bike collided ...\n",
              "7612    The Latest: More Homes Razed by Northern Calif...\n",
              "Name: text, Length: 7613, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cython \n",
        "# cython: linetrace=True\n",
        "# cython: binding=True\n",
        "# distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
        "# cython: profile=True\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import itertools\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "cdef seed_everything(int seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(42)\n",
        "cdef int MAX_LENGTH = 30\n",
        "cdef object device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cdef clean_sentence(object sentence):\n",
        "    # remove URLS \n",
        "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
        "    # remove emoji's\n",
        "    cdef object emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    sentence = emoji_pattern.sub(r' ', sentence)\n",
        "\n",
        "    # remove punctuation\n",
        "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
        "    # remove double spaces\n",
        "    sentence = sentence.replace('  ',\"\")\n",
        "    \n",
        "    return sentence.strip()\n",
        "\n",
        "cdef remove_stopwords(list tokens, object stopwords):\n",
        "    cdef list clean_token = [word for word in tokens if word not in stopwords]\n",
        "    return clean_token\n",
        "\n",
        "# lemmalization \n",
        "cdef lemmatize(list tokens, object lemma):\n",
        "    cdef list lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "cdef trunc_padding(list sentence):\n",
        "    sentence = list(itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH)) # fix sentence length\n",
        "    return sentence\n",
        "\n",
        "cdef object stopwords = nltk.corpus.stopwords.words('english')\n",
        "cdef object lemma = WordNetLemmatizer()\n",
        "\n",
        "def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
        "    df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
        "    df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
        "    # tokenization 123123\n",
        "    df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
        "    # remove stopwords\n",
        "    df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    # lemmalization \n",
        "    df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
        "    # sentence length before padding\n",
        "    df['length'] = df['text'].map(lambda x: len(x))\n",
        "    # fix sentence length\n",
        "    df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
        "    # sentence length after padding\n",
        "    df['length_padding'] = df['text'].map(lambda x: len(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNvP5cFlmo1H",
        "outputId": "b767a3c1-bc3d-4735-8534-62f9458d997a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "lemma = WordNetLemmatizer()\n",
        "%lprun -f processing processing(train, stopwords, lemma)"
      ],
      "metadata": {
        "id": "2nSbrGYwnIlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8602d040-b9e6-40de-89a5-1c83df108036"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "Timer unit: 1e-06 s\n",
        "\n",
        "Total time: 6.21616 s\n",
        "File: /root/.cache/ipython/cython/_cython_magic_8cec306dd1bca0d781798f8b4263af96.pyx\n",
        "Function: processing at line 76\n",
        "\n",
        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
        "==============================================================\n",
        "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
        "    77         1      28608.0  28608.0      0.5      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
        "    78         1     585561.0 585561.0      9.4      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
        "    79                                               # tokenization 123123\n",
        "    80         1      17020.0  17020.0      0.3      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
        "    81                                               # remove stopwords\n",
        "    82         1      20294.0  20294.0      0.3      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    83                                               # lemmalization \n",
        "    84         1    5542541.0 5542541.0     89.2      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
        "    85                                               # sentence length before padding\n",
        "    86         1       4007.0   4007.0      0.1      df['length'] = df['text'].map(lambda x: len(x))\n",
        "    87                                               # fix sentence length\n",
        "    88         1      14625.0  14625.0      0.2      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
        "    89                                               # sentence length after padding\n",
        "    90         1       3506.0   3506.0      0.1      df['length_padding'] = df['text'].map(lambda x: len(x))"
      ],
      "metadata": {
        "id": "Qs-IoIJuwJ3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext line_profiler\n",
        "%lprun -f processing processing(test, stopwords, lemma)"
      ],
      "metadata": {
        "id": "iOnVIG-Kqywg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<pre>Timer unit: 1e-06 s\n",
        "\n",
        "Total time: 0.517537 s\n",
        "File: /root/.cache/ipython/cython/_cython_magic_8cec306dd1bca0d781798f8b4263af96.pyx\n",
        "Function: processing at line 76\n",
        "\n",
        "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
        "==============================================================\n",
        "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
        "    77         1       4426.0   4426.0      0.9      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
        "    78         1      40446.0  40446.0      7.8      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
        "    79                                               # tokenization 123123\n",
        "    80         1       5095.0   5095.0      1.0      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
        "    81                                               # remove stopwords\n",
        "    82         1       6172.0   6172.0      1.2      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
        "    83                                               # lemmalization \n",
        "    84         1     451740.0 451740.0     87.3      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
        "    85                                               # sentence length before padding\n",
        "    86         1       1992.0   1992.0      0.4      df['length'] = df['text'].map(lambda x: len(x))\n",
        "    87                                               # fix sentence length\n",
        "    88         1       5861.0   5861.0      1.1      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
        "    89                                               # sentence length after padding\n",
        "    90         1       1805.0   1805.0      0.3      df['length_padding'] = df['text'].map(lambda x: len(x))"
      ],
      "metadata": {
        "id": "rNuWPuSQwM20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison\n",
        "processing training data without optimization takes:  7.58081 s <br>\n",
        "processing training data with Cython optimization takes:  6.21616 s <br>\n",
        "The speed up is : 7.58081 / 6.21616 ≈ 1.22 <br>\n",
        "The relative speed up is 22 %\n",
        "<br><br>\n",
        "processing testing data without optimization takes:  0.59442 s <br>\n",
        "processing testing data with Cython optimization takes:  0.517537 s <br>\n",
        "The speed up is : 0.59442 / 0.517537 ≈ 1.15<br>\n",
        "The relative speed up is 15 %\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlB2L-DLwVe0"
      }
    }
  ]
}