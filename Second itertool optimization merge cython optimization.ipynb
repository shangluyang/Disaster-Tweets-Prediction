{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without any optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "MAX_LENGTH = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # remove URLS \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "    # remove emoji's\n",
    "    sentence = remove_emoji(sentence)\n",
    "    # remove punctuation\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
    "    # remove double spaces\n",
    "    sentence = sentence.replace('  ',\"\")\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    clean_token = [word for word in tokens if word not in stopwords]\n",
    "    return clean_token\n",
    "\n",
    "# lemmalization \n",
    "def lemmatize(tokens, lemma):\n",
    "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    modify_sentence = sentence.copy()\n",
    "    if len(modify_sentence) >= MAX_LENGTH:\n",
    "        modify_sentence = modify_sentence[:MAX_LENGTH]\n",
    "    else:\n",
    "        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n",
    "    return modify_sentence\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def processing(df, stopwords, lemma):\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    # tokenization\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
    "    # lemmalization \n",
    "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
    "    # sentence length before padding\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "    # fix sentence length\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    # sentence length after padding\n",
    "    df['length_padding'] = df['text'].apply(lambda x: len(x))\n",
    "\n",
    "train= pd.read_csv('train.csv')\n",
    "test= pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 3.53028 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_6460\\4085473941.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    78                                           \n",
      "    79         1      50787.0  50787.0      0.1  \n",
      "    80         1     921635.0 921635.0      2.6  \n",
      "    81                                           \n",
      "    82         1      89379.0  89379.0      0.3  \n",
      "    83                                           \n",
      "    84         1    1401477.0 1401477.0      4.0  \n",
      "    85                                           \n",
      "    86         1   32633575.0 32633575.0     92.4  \n",
      "    87                                           \n",
      "    88         1      32938.0  32938.0      0.1  \n",
      "    89                                           \n",
      "    90         1     141000.0 141000.0      0.4  \n",
      "    91                                           \n",
      "    92         1      31974.0  31974.0      0.1"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(train, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.39762 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_6460\\4085473941.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    78                                           \n",
      "    79         1      42374.0  42374.0      1.1  \n",
      "    80         1     373807.0 373807.0      9.4  \n",
      "    81                                           \n",
      "    82         1      75361.0  75361.0      1.9  \n",
      "    83                                           \n",
      "    84         1     672020.0 672020.0     16.9  \n",
      "    85                                           \n",
      "    86         1    2730069.0 2730069.0     68.7  \n",
      "    87                                           \n",
      "    88         1      17505.0  17505.0      0.4  \n",
      "    89                                           \n",
      "    90         1      49118.0  49118.0      1.2  \n",
      "    91                                           \n",
      "    92         1      15942.0  15942.0      0.4"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test, stopwords, lemma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Itertool optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "MAX_LENGTH = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train= pd.read_csv('train.csv')\n",
    "test= pd.read_csv(\"test.csv\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', sentence)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# combine func clean_sentence,remove_stopwords,lemmatize \n",
    "def clean_sentence(sentence): \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence) # remove urls\n",
    "    sentence = remove_emoji(sentence) # remove emojis\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\", sentence) # remove punctuations\n",
    "    sentence = sentence.replace('  ',\"\") # remove double spaces\n",
    "    sentence = sentence.lower().split() # lowercase and tokenize\n",
    "    sentence = itertools.filterfalse(lambda x: x in stopwords, sentence) # remove stopwords\n",
    "    sentence = list(itertools.chain.from_iterable(map(lambda x: [lemma.lemmatize(x, pos='v')], sentence))) # lemmatize\n",
    "    return sentence\n",
    "\n",
    "#fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    sentence = itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH) # fix sentence length\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "def processing(df):\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    df['length_padding'] = MAX_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 3.40684 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_12120\\1775499896.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    76                                           \n",
      "    77         1   33876578.0 33876578.0     99.4  \n",
      "    78         1      34907.0  34907.0      0.1  \n",
      "    79         1     152833.0 152833.0      0.4  \n",
      "    80         1       4034.0   4034.0      0.0"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.363519 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_12120\\1775499896.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    76                                           \n",
      "    77         1    3529557.0 3529557.0     97.1  \n",
      "    78         1      18275.0  18275.0      0.5  \n",
      "    79         1      83305.0  83305.0      2.3  \n",
      "    80         1       4058.0   4058.0      0.1"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Itertool optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "MAX_LENGTH = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train= pd.read_csv('./train.csv')\n",
    "test= pd.read_csv(\"./test.csv\")\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# combine func clean_sentence,remove_stopwords,lemmatize \n",
    "def clean_sentence(sentence): \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence) # remove urls\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r' ', sentence)\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\", sentence) # remove punctuations\n",
    "    sentence = sentence.replace('  ',\"\") # remove double spaces\n",
    "    sentence = sentence.lower().split() # lowercase and tokenize\n",
    "    sentence = (lemma.lemmatize(x, pos='v') for x in sentence if x not in stopwords) # lemmatize and remove stopwords using generator expression\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "#fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    sentence = itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH) # fix sentence length\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "def processing(df):\n",
    "    df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
    "    df['length'] = df['text'].map(lambda x: len(x))\n",
    "    df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
    "    df['length_padding'] = MAX_LENGTH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 3.36837 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_12136\\2561017597.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    74                                           \n",
      "    75         1   33512587.0 33512587.0     99.5  \n",
      "    76         1      31706.0  31706.0      0.1  \n",
      "    77         1     135410.0 135410.0      0.4  \n",
      "    78         1       4010.0   4010.0      0.0"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.34409 s\n",
      "\n",
      "Could not find file C:\\Users\\czy\\AppData\\Local\\Temp\\ipykernel_12136\\2561017597.py\n",
      "Are you sure you are running this program from the same directory\n",
      "that you ran the profiler from?\n",
      "Continuing without the function's contents.\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    74                                           \n",
      "    75         1    3299331.0 3299331.0     95.9  \n",
      "    76         1      28105.0  28105.0      0.8  \n",
      "    77         1     108751.0 108751.0      3.2  \n",
      "    78         1       4716.0   4716.0      0.1"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Reduce unnecessary function calls and merge multiple functions into a few. Python function calls are an expensive operation as they require passing parameters between functions and jumping to other parts of the code. Reducing unnecessary function calls can reduce this additional overhead and improve program performance.\n",
    "\n",
    "Method 2: Using the map() function in the Pandas library can optimize Python program performance. When processing DataFrame objects, apply() is a common method. However, map() is usually faster than apply() because it does not call Python functions like apply() does. Instead, it utilizes built-in optimization techniques to process data.\n",
    "\n",
    "Method 3: Using generator expressions can further improve program performance. Generator expressions are a concise and efficient way to handle lists or other iterable objects. Compared to using map() and filter() functions, generator expressions can calculate each value all at once, rather than loading the entire list into memory, reducing memory overhead and improving program performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Itertool optimization with Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%cython \n",
    "# cython: linetrace=True\n",
    "# cython: binding=True\n",
    "# distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
    "# cython: profile=True\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import itertools\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "cdef seed_everything(int seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "cdef int MAX_LENGTH = 30\n",
    "cdef object device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cdef clean_sentence(object sentence):\n",
    "    # remove URLS \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "    # remove emoji's\n",
    "    cdef object emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r' ', sentence)\n",
    "\n",
    "    # remove punctuation\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
    "    # remove double spaces\n",
    "    sentence = sentence.replace('  ',\"\")\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "cdef remove_stopwords(list tokens, object stopwords):\n",
    "    cdef list clean_token = [word for word in tokens if word not in stopwords]\n",
    "    return clean_token\n",
    "\n",
    "# lemmalization \n",
    "cdef lemmatize(list tokens, object lemma):\n",
    "    cdef list lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "cdef trunc_padding(list sentence):\n",
    "    sentence = list(itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH)) # fix sentence length\n",
    "    return sentence\n",
    "\n",
    "cdef object stopwords = nltk.corpus.stopwords.words('english')\n",
    "cdef object lemma = WordNetLemmatizer()\n",
    "\n",
    "def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
    "    df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
    "    df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
    "    # tokenization 123123\n",
    "    df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
    "    # lemmalization \n",
    "    df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
    "    # sentence length before padding\n",
    "    df['length'] = df['text'].map(lambda x: len(x))\n",
    "    # fix sentence length\n",
    "    df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
    "    # sentence length after padding\n",
    "    df['length_padding'] = df['text'].map(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 3.25333 s\n",
      "File: C:\\Users\\czy\\.ipython\\cython\\_cython_magic_9b41f3ce78be3cc6a616579f23709c5e.pyx\n",
      "Function: processing at line 76\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
      "    77         1      52813.0  52813.0      0.2      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
      "    78         1     847476.0 847476.0      2.6      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
      "    79                                               # tokenization 123123\n",
      "    80         1      73099.0  73099.0      0.2      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
      "    81                                               # remove stopwords\n",
      "    82         1     135215.0 135215.0      0.4      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
      "    83                                               # lemmalization \n",
      "    84         1   31262069.0 31262069.0     96.1      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
      "    85                                               # sentence length before padding\n",
      "    86         1      24892.0  24892.0      0.1      df['length'] = df['text'].map(lambda x: len(x))\n",
      "    87                                               # fix sentence length\n",
      "    88         1     110098.0 110098.0      0.3      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
      "    89                                               # sentence length after padding\n",
      "    90         1      27612.0  27612.0      0.1      df['length_padding'] = df['text'].map(lambda x: len(x))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "train= pd.read_csv('./train.csv')\n",
    "test= pd.read_csv(\"./test.csv\")\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "%lprun -f processing processing(train, stopwords, lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.319202 s\n",
      "File: C:\\Users\\czy\\.ipython\\cython\\_cython_magic_9b41f3ce78be3cc6a616579f23709c5e.pyx\n",
      "Function: processing at line 76\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    76                                           def processing(df: pd.DataFrame, object stopwords, object lemma):\n",
      "    77         1      33561.0  33561.0      1.1      df['text'] = df['text'].map(lambda sentence: sentence.lower())\n",
      "    78         1     297020.0 297020.0      9.3      df['text'] = df['text'].map(lambda sentence: clean_sentence(sentence))\n",
      "    79                                               # tokenization 123123\n",
      "    80         1      89781.0  89781.0      2.8      df['text'] = df['text'].map(lambda sentence: sentence.split())\n",
      "    81                                               # remove stopwords\n",
      "    82         1      39306.0  39306.0      1.2      df['text'] = df['text'].map(lambda sentence: remove_stopwords(sentence, stopwords))\n",
      "    83                                               # lemmalization \n",
      "    84         1    2664646.0 2664646.0     83.5      df['text'] = df['text'].map(lambda sentence: lemmatize(sentence, lemma))\n",
      "    85                                               # sentence length before padding\n",
      "    86         1      13063.0  13063.0      0.4      df['length'] = df['text'].map(lambda x: len(x))\n",
      "    87                                               # fix sentence length\n",
      "    88         1      42024.0  42024.0      1.3      df['text'] = df['text'].map(lambda sentence: trunc_padding(sentence))\n",
      "    89                                               # sentence length after padding\n",
      "    90         1      12618.0  12618.0      0.4      df['length_padding'] = df['text'].map(lambda x: len(x))"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test, stopwords, lemma)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without any optimization:<br>\n",
    "Total time for processing training data: 3.53028 seconds.<br>\n",
    "Total time for processing testing data: 0.39762 seconds.<br>\n",
    "\n",
    "First Itertool optimization:<br>\n",
    "Total time for processing training data: 3.40684 seconds, an improvement of about 3.5% relative to the previous time.<br>\n",
    "Total time for processing testing data: 0.363519 seconds, an improvement of about 8.9% relative to the previous time.<br>\n",
    "\n",
    "Second Itertool optimization:<br>\n",
    "Total time for processing training data: 3.36837 seconds, an improvement of about 1.1% relative to the previous time.<br>\n",
    "Total time for processing testing data: 0.34409 seconds, an improvement of about 5.3% relative to the previous time.<br>\n",
    "\n",
    "Second Itertool optimization with Cython:<br>\n",
    "Total time for processing training data: 3.25333 seconds, an improvement of about 3.4% relative to the previous time.<br>\n",
    "Total time for processing testing data: 0.319202 seconds, an improvement of about 7.0% relative to the previous time.<br>\n",
    "\n",
    "The percentage improvement relative to the original data can be calculated by comparing the total times for processing training and testing data between the original and final optimizations:\n",
    "\n",
    "- Total time for processing training data improved from 3.53028 seconds to 3.25333 seconds, which is a percentage improvement of approximately (1 - 3.25333/3.53028) * 100% = 7.8%.\n",
    "- Total time for processing testing data improved from 0.39762 seconds to 0.319202 seconds, which is a percentage improvement of approximately (1 - 0.319202/0.39762) * 100% = 19.7%.\n",
    "\n",
    "Overall, the final optimization with Cython and the second Itertool optimization achieved a total percentage improvement of approximately 7.8% for processing training data and 19.7% for processing testing data relative to the original processing times.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
